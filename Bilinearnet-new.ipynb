{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bilinearnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aw8EeKsuA9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xueIX7hJuJdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "import os\n",
        "os.chdir(\"drive/ML/分类/\")\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leED-ejvukA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "images=[]\n",
        "labels=[]\n",
        "def read_image(imageName):\n",
        "    im = Image.open(imageName).convert('RGB')\n",
        "    data = np.array(im)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMvkHIgcukm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 读取在words里面有几个文件夹\n",
        "text = os.listdir('./resize_data')\n",
        "# 把文件夹里面的图片和其对应的文件夹的名字也就是对应的字\n",
        "for textPath in text:\n",
        "    for fn in os.listdir(os.path.join('resize_data', textPath)):\n",
        "        if fn.endswith('.jpg'):\n",
        "            fd = os.path.join('./resize_data', textPath, fn)\n",
        "            i=0\n",
        "            print(i)\n",
        "            i=i+1\n",
        "            images.append(read_image(fd))\n",
        "            labels.append(textPath)\n",
        "            \n",
        "X = np.array(images)\n",
        "y = np.array(list(map(int, labels))) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwe4kItLumNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjObF78SuoDV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d9b0b343-95cf-4b82-a896-503dad1ea6b4"
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "from keras.utils import np_utils\n",
        "Y_train = np_utils.to_categorical(y_train, 8)\n",
        "Y_test = np_utils.to_categorical(y_test, 8)\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3200, 224, 224, 3)\n",
            "(3200, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PXZYamejqay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "def Precision(y_true, y_pred):\n",
        "    \"\"\"精确率\"\"\"\n",
        "    tp= K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))  # true positives\n",
        "    pp= K.sum(K.round(K.clip(y_pred, 0, 1))) # predicted positives\n",
        "    precision = tp/ (pp+ K.epsilon())\n",
        "    return precision\n",
        "    \n",
        "def Recall(y_true, y_pred):\n",
        "    \"\"\"召回率\"\"\"\n",
        "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) # true positives\n",
        "    pp = K.sum(K.round(K.clip(y_true, 0, 1))) # possible positives\n",
        "    recall = tp / (pp + K.epsilon())\n",
        "    return recall\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5hP9eNlusAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Convolution2D,Activation,MaxPooling2D,Flatten,Dense,Dropout,Input,Reshape,Lambda\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model\n",
        "import numpy as np\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,LearningRateScheduler,ReduceLROnPlateau\n",
        " \n",
        "def sign_sqrt(x):\n",
        "    return K.sign(x) * K.sqrt(K.abs(x) + 1e-10)\n",
        "\n",
        "def l2_norm(x):\n",
        "    return K.l2_normalize(x, axis=-1)\n",
        "\n",
        "def batch_dot(cnn_ab):\n",
        "    return K.batch_dot(cnn_ab[0], cnn_ab[1], axes=[1, 1])\n",
        "\n",
        "def bilinearnet():\n",
        "    input_tensor = Input(shape=(224,224,3))\n",
        "    vgg16 = VGG16(weights='imagenet', include_top=False,input_tensor=input_tensor)\n",
        "#    conv2048 = Convolution2D(filters=2048,kernel_size=(3,3),)\n",
        "#    vgg16_add_conv_to_2048 = Model(inputs=input_tensor,outputs=)\n",
        "    resnet50 = ResNet50(weights='imagenet', include_top=False,input_tensor=input_tensor)\n",
        "    model_vgg16 = Model(inputs=input_tensor,outputs=vgg16.output)\n",
        "    model_resnet50 = Model(inputs=input_tensor,outputs=resnet50.output)\n",
        "    model_vgg16.compile(loss='categorical_crossentropy',optimizer='adam')\n",
        "    model_resnet50.compile(loss='categorical_crossentropy',optimizer='adam')\n",
        "    \n",
        "    resnet50_x = Reshape([model_resnet50.layers[-6].output_shape[1]*model_resnet50.layers[-6].output_shape[2],\n",
        "                          model_resnet50.layers[-6].output_shape[3]])(model_resnet50.layers[-6].output)\n",
        "    vgg16_x = Reshape([model_vgg16.layers[-1].output_shape[1]*model_vgg16.layers[-1].output_shape[2],\n",
        "                       model_vgg16.layers[-1].output_shape[3]])(model_vgg16.layers[-1].output)\n",
        "    \n",
        "    cnn_dot_out = Lambda(batch_dot)([vgg16_x,resnet50_x])\n",
        "    \n",
        "    sign_sqrt_out = Lambda(sign_sqrt)(cnn_dot_out)\n",
        "    l2_norm_out = Lambda(l2_norm)(sign_sqrt_out)\n",
        "    flatten = Flatten()(l2_norm_out)\n",
        "    dropout = Dropout(0.5)(flatten)\n",
        "    output = Dense(8, activation='softmax')(dropout)\n",
        "    \n",
        "    model = Model(input_tensor, output)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9, decay=1e-6),\n",
        "                  metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHmoMNP2y5Tc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=bilinearnet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV3kT7COzu-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "model_checkpoint = ModelCheckpoint('Bilinearnet.h5', monitor='val_loss', verbose=1, save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GOKex3MkaOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history=model.fit(X_train, Y_train, batch_size=50, epochs=50, verbose=1,\n",
        "          validation_data=(X_test, Y_test),callbacks=[model_checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3mZzVhrKUdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(history.history.keys())\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmz3q9CQ_p9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "import keras.layers\n",
        "import keras.applications\n",
        "import keras.backend\n",
        "import keras.preprocessing.image\n",
        "import keras.utils\n",
        "import tensorflow as tf\n",
        "\n",
        "import cv2\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.initializers import glorot_normal\n",
        "\n",
        "def outer_product(x):\n",
        "    \"\"\"\n",
        "    calculate outer-products of 2 tensors\n",
        "\n",
        "        args \n",
        "            x\n",
        "                list of 2 tensors\n",
        "                , assuming each of which has shape = (size_minibatch, total_pixels, size_filter)\n",
        "    \"\"\"\n",
        "    return keras.backend.batch_dot(\n",
        "                x[0]\n",
        "                , x[1]\n",
        "                , axes=[1,1]\n",
        "            ) / x[0].get_shape().as_list()[1] \n",
        "\n",
        "def signed_sqrt(x):\n",
        "    \"\"\"\n",
        "    calculate element-wise signed square root\n",
        "\n",
        "        args\n",
        "            x\n",
        "                a tensor\n",
        "    \"\"\"\n",
        "    return keras.backend.sign(x) * keras.backend.sqrt(keras.backend.abs(x) + 1e-9)\n",
        "\n",
        "def L2_norm(x, axis=-1):\n",
        "    \"\"\"\n",
        "    calculate L2-norm\n",
        "\n",
        "        args \n",
        "            x\n",
        "                a tensor\n",
        "    \"\"\"\n",
        "    return keras.backend.l2_normalize(x, axis=axis)\n",
        "\n",
        "\n",
        "def build_model(\n",
        "    size_heigth=224\n",
        "    ,size_width=224\n",
        "    ,no_class=8\n",
        "    ,no_last_layer_backbone=17\n",
        "    \n",
        "    ,name_optimizer=\"sgd\"\n",
        "    ,rate_learning=1.0\n",
        "    ,rate_decay_learning=0.0\n",
        "    ,rate_decay_weight=0.0\n",
        "    \n",
        "    ,name_initializer=\"glorot_normal\"\n",
        "    ,name_activation_logits=\"softmax\"\n",
        "    ,name_loss=\"categorical_crossentropy\"\n",
        "\n",
        "    ,flg_debug=False\n",
        "    ,**kwargs\n",
        "):\n",
        "    \n",
        "    keras.backend.clear_session()\n",
        "    \n",
        "    print(\"-------------------------------\")\n",
        "    print(\"parameters:\")\n",
        "    for key, val in locals().items():\n",
        "        if not val == None and not key == \"kwargs\":\n",
        "            print(\"\\t\", key, \"=\",  val)\n",
        "    print(\"-------------------------------\")\n",
        "    \n",
        "    ### \n",
        "    ### load pre-trained model\n",
        "    ###\n",
        "    tensor_input = keras.layers.Input(shape=[size_heigth,size_width,3])\n",
        "    model_detector = keras.applications.vgg16.VGG16(\n",
        "                            input_tensor=tensor_input\n",
        "                            , include_top=False\n",
        "                            , weights='imagenet'\n",
        "                        )\n",
        "    \n",
        "\n",
        "    ### \n",
        "    ### bi-linear pooling\n",
        "    ###\n",
        "\n",
        "    # extract features from detector\n",
        "    x_detector = model_detector.layers[no_last_layer_backbone].output\n",
        "    shape_detector = model_detector.layers[no_last_layer_backbone].output_shape\n",
        "    if flg_debug:\n",
        "        print(\"shape_detector : {}\".format(shape_detector))\n",
        "\n",
        "    # extract features from extractor , same with detector for symmetry DxD model\n",
        "    shape_extractor = shape_detector\n",
        "    x_extractor = x_detector\n",
        "    if flg_debug:\n",
        "        print(\"shape_extractor : {}\".format(shape_extractor))\n",
        "        \n",
        "    \n",
        "    # rehape to (minibatch_size, total_pixels, filter_size)\n",
        "    x_detector = keras.layers.Reshape(\n",
        "            [\n",
        "                shape_detector[1] * shape_detector[2] , shape_detector[-1]\n",
        "            ]\n",
        "        )(x_detector)\n",
        "    if flg_debug:\n",
        "        print(\"x_detector shape after rehsape ops : {}\".format(x_detector.shape))\n",
        "        \n",
        "    x_extractor = keras.layers.Reshape(\n",
        "            [\n",
        "                shape_extractor[1] * shape_extractor[2] , shape_extractor[-1]\n",
        "            ]\n",
        "        )(x_extractor)\n",
        "    if flg_debug:\n",
        "        print(\"x_extractor shape after rehsape ops : {}\".format(x_extractor.shape))\n",
        "        \n",
        "        \n",
        "    # outer products of features, output shape=(minibatch_size, filter_size_detector*filter_size_extractor)\n",
        "    x = keras.layers.Lambda(outer_product)(\n",
        "        [x_detector, x_extractor]\n",
        "    )\n",
        "    if flg_debug:\n",
        "        print(\"x shape after outer products ops : {}\".format(x.shape))\n",
        "        \n",
        "        \n",
        "    # rehape to (minibatch_size, filter_size_detector*filter_size_extractor)\n",
        "    x = keras.layers.Reshape([shape_detector[-1]*shape_extractor[-1]])(x)\n",
        "    if flg_debug:\n",
        "        print(\"x shape after rehsape ops : {}\".format(x.shape))\n",
        "        \n",
        "        \n",
        "    # signed square-root \n",
        "    x = keras.layers.Lambda(signed_sqrt)(x)\n",
        "    if flg_debug:\n",
        "        print(\"x shape after signed-square-root ops : {}\".format(x.shape))\n",
        "        \n",
        "    # L2 normalization\n",
        "    x = keras.layers.Lambda(L2_norm)(x)\n",
        "    if flg_debug:\n",
        "        print(\"x shape after L2-Normalization ops : {}\".format(x.shape))\n",
        "\n",
        "\n",
        "\n",
        "    ### \n",
        "    ### attach FC-Layer\n",
        "    ###\n",
        "\n",
        "    if name_initializer != None:\n",
        "            name_initializer = eval(name_initializer+\"()\")\n",
        "            \n",
        "    x = keras.layers.Dense(\n",
        "            units=no_class\n",
        "            ,kernel_regularizer=keras.regularizers.l2(rate_decay_weight)\n",
        "            ,kernel_initializer=name_initializer\n",
        "        )(x)\n",
        "    if flg_debug:\n",
        "        print(\"x shape after Dense ops : {}\".format(x.shape))\n",
        "    tensor_prediction = keras.layers.Activation(name_activation_logits)(x)\n",
        "    if flg_debug:\n",
        "        print(\"prediction shape : {}\".format(tensor_prediction.shape))\n",
        "\n",
        "        \n",
        "\n",
        "    ### \n",
        "    ### compile model\n",
        "    ###\n",
        "    model_bilinear = keras.models.Model(\n",
        "                        inputs=[tensor_input]\n",
        "                        , outputs=[tensor_prediction]\n",
        "                    )\n",
        "    \n",
        "    \n",
        "    # fix pre-trained weights\n",
        "    for layer in model_detector.layers:\n",
        "        layer.trainable = False\n",
        "        \n",
        "        \n",
        "    # define optimizers\n",
        "    opt_adam = keras.optimizers.adam(\n",
        "                    lr=rate_learning\n",
        "                    , decay=rate_decay_learning\n",
        "                )\n",
        "    opt_rms = keras.optimizers.RMSprop(\n",
        "                    lr=rate_learning\n",
        "                    , decay=rate_decay_learning\n",
        "                )\n",
        "    opt_sgd = keras.optimizers.SGD(\n",
        "                    lr=rate_learning\n",
        "                    , decay=rate_decay_learning\n",
        "                    , momentum=0.9\n",
        "                    , nesterov=False\n",
        "                )\n",
        "    optimizers ={\n",
        "        \"adam\":opt_adam\n",
        "        ,\"rmsprop\":opt_rms\n",
        "        ,\"sgd\":opt_sgd\n",
        "    }\n",
        "    \n",
        "    model_bilinear.compile(\n",
        "        loss=name_loss\n",
        "        , optimizer=optimizers[name_optimizer]\n",
        "        , metrics=[\"categorical_accuracy\"]\n",
        "    )\n",
        "    \n",
        "    \n",
        "    \n",
        "    if flg_debug:\n",
        "        model_bilinear.summary()\n",
        "    \n",
        "    return model_bilinear"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwdbn6up_yL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "            # number of output classes, 8 for CUB200\n",
        "            no_class = 8\n",
        "\n",
        "            # pretrained model specification, using VGG16\n",
        "            # \"block5_conv3 \"\n",
        "            ,no_last_layer_backbone = 17\n",
        "    \n",
        "            # training parametes\n",
        "            ,rate_learning=1.0\n",
        "            ,rate_decay_weight=1e-8\n",
        "    \n",
        "            ,flg_debug=True\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RN0hZ1KAeGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "model_checkpoint = ModelCheckpoint('vgg-Bilinearnet.h5', monitor='loss', verbose=1, save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W7lwdMeghBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history=model.fit(X_train, Y_train, batch_size=50, epochs=50, verbose=1,\n",
        "          validation_data=(X_test, Y_test),callbacks=[model_checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODTJMx7WSUTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.models import Sequential, Model\n",
        "model.load_weights('vgg-Bilinearnet.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HwarbEX4Ka0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now all layers are trainable\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# change LR\n",
        "opt_sgd = keras.optimizers.SGD(\n",
        "                lr=1e-3\n",
        "                , decay=1e-9\n",
        "                , momentum=0.9\n",
        "                , nesterov=False\n",
        "            )\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\"\n",
        "    , optimizer=opt_sgd\n",
        "    , metrics=[\"categorical_accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmZHebhF-MUv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16679074-d0e5-4e7c-e9e4-5b91d48b03cb"
      },
      "source": [
        "history=model.fit(X_train, Y_train, batch_size=50, epochs=100, verbose=1,\n",
        "          validation_data=(X_test, Y_test),callbacks=[model_checkpoint])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3200 samples, validate on 800 samples\n",
            "Epoch 1/100\n",
            "3200/3200 [==============================] - 71s 22ms/step - loss: 0.1465 - categorical_accuracy: 0.9484 - val_loss: 0.2123 - val_categorical_accuracy: 0.9187\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.14654, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 2/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0766 - categorical_accuracy: 0.9750 - val_loss: 0.1176 - val_categorical_accuracy: 0.9575\n",
            "\n",
            "Epoch 00002: loss improved from 0.14654 to 0.07657, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 3/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0370 - categorical_accuracy: 0.9906 - val_loss: 0.0727 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00003: loss improved from 0.07657 to 0.03696, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 4/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0129 - categorical_accuracy: 0.9994 - val_loss: 0.0683 - val_categorical_accuracy: 0.9737\n",
            "\n",
            "Epoch 00004: loss improved from 0.03696 to 0.01289, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 5/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0063 - categorical_accuracy: 0.9997 - val_loss: 0.0705 - val_categorical_accuracy: 0.9738\n",
            "\n",
            "Epoch 00005: loss improved from 0.01289 to 0.00633, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 6/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0040 - categorical_accuracy: 1.0000 - val_loss: 0.0684 - val_categorical_accuracy: 0.9750\n",
            "\n",
            "Epoch 00006: loss improved from 0.00633 to 0.00398, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 7/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0033 - categorical_accuracy: 1.0000 - val_loss: 0.0644 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00007: loss improved from 0.00398 to 0.00330, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 8/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0025 - categorical_accuracy: 1.0000 - val_loss: 0.0689 - val_categorical_accuracy: 0.9763\n",
            "\n",
            "Epoch 00008: loss improved from 0.00330 to 0.00249, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 9/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0022 - categorical_accuracy: 1.0000 - val_loss: 0.0674 - val_categorical_accuracy: 0.9750\n",
            "\n",
            "Epoch 00009: loss improved from 0.00249 to 0.00219, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 10/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0020 - categorical_accuracy: 1.0000 - val_loss: 0.0725 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00010: loss improved from 0.00219 to 0.00195, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 11/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0018 - categorical_accuracy: 1.0000 - val_loss: 0.0676 - val_categorical_accuracy: 0.9775\n",
            "\n",
            "Epoch 00011: loss improved from 0.00195 to 0.00181, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 12/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0017 - categorical_accuracy: 1.0000 - val_loss: 0.0697 - val_categorical_accuracy: 0.9750\n",
            "\n",
            "Epoch 00012: loss improved from 0.00181 to 0.00166, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 13/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 0.0699 - val_categorical_accuracy: 0.9763\n",
            "\n",
            "Epoch 00013: loss improved from 0.00166 to 0.00154, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 14/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 0.0693 - val_categorical_accuracy: 0.9763\n",
            "\n",
            "Epoch 00014: loss improved from 0.00154 to 0.00145, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 15/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0014 - categorical_accuracy: 1.0000 - val_loss: 0.0691 - val_categorical_accuracy: 0.9763\n",
            "\n",
            "Epoch 00015: loss improved from 0.00145 to 0.00138, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 16/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0013 - categorical_accuracy: 1.0000 - val_loss: 0.0698 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00016: loss improved from 0.00138 to 0.00130, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 17/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 0.0726 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00017: loss improved from 0.00130 to 0.00124, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 18/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 0.0711 - val_categorical_accuracy: 0.9775\n",
            "\n",
            "Epoch 00018: loss improved from 0.00124 to 0.00119, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 19/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0011 - categorical_accuracy: 1.0000 - val_loss: 0.0722 - val_categorical_accuracy: 0.9750\n",
            "\n",
            "Epoch 00019: loss improved from 0.00119 to 0.00114, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 20/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0011 - categorical_accuracy: 1.0000 - val_loss: 0.0708 - val_categorical_accuracy: 0.9775\n",
            "\n",
            "Epoch 00020: loss improved from 0.00114 to 0.00110, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 21/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0011 - categorical_accuracy: 1.0000 - val_loss: 0.0708 - val_categorical_accuracy: 0.9763\n",
            "\n",
            "Epoch 00021: loss improved from 0.00110 to 0.00106, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 22/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 0.0010 - categorical_accuracy: 1.0000 - val_loss: 0.0724 - val_categorical_accuracy: 0.9763\n",
            "\n",
            "Epoch 00022: loss improved from 0.00106 to 0.00102, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 23/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 9.9437e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0725 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00023: loss improved from 0.00102 to 0.00099, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 24/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 9.6748e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0722 - val_categorical_accuracy: 0.9763\n",
            "\n",
            "Epoch 00024: loss improved from 0.00099 to 0.00097, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 25/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 9.3875e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0719 - val_categorical_accuracy: 0.9775\n",
            "\n",
            "Epoch 00025: loss improved from 0.00097 to 0.00094, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 26/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 9.1352e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0739 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00026: loss improved from 0.00094 to 0.00091, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 27/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 8.9211e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0732 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00027: loss improved from 0.00091 to 0.00089, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 28/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 8.7325e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0731 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00028: loss improved from 0.00089 to 0.00087, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 29/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 8.5387e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0730 - val_categorical_accuracy: 0.9762\n",
            "\n",
            "Epoch 00029: loss improved from 0.00087 to 0.00085, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 30/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 8.3228e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0744 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00030: loss improved from 0.00085 to 0.00083, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 31/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 8.1629e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0741 - val_categorical_accuracy: 0.9775\n",
            "\n",
            "Epoch 00031: loss improved from 0.00083 to 0.00082, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 32/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 7.9942e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0735 - val_categorical_accuracy: 0.9775\n",
            "\n",
            "Epoch 00032: loss improved from 0.00082 to 0.00080, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 33/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 7.8479e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0750 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00033: loss improved from 0.00080 to 0.00078, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 34/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 7.7053e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0738 - val_categorical_accuracy: 0.9775\n",
            "\n",
            "Epoch 00034: loss improved from 0.00078 to 0.00077, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 35/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 7.5994e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0740 - val_categorical_accuracy: 0.9775\n",
            "\n",
            "Epoch 00035: loss improved from 0.00077 to 0.00076, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 36/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 7.4495e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0754 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00036: loss improved from 0.00076 to 0.00074, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 37/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 7.3099e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0751 - val_categorical_accuracy: 0.9775\n",
            "\n",
            "Epoch 00037: loss improved from 0.00074 to 0.00073, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 38/100\n",
            "3200/3200 [==============================] - 56s 18ms/step - loss: 7.1967e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0760 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00038: loss improved from 0.00073 to 0.00072, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 39/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 7.0951e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0768 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00039: loss improved from 0.00072 to 0.00071, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 40/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 6.9860e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0772 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00040: loss improved from 0.00071 to 0.00070, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 41/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 6.9090e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0763 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00041: loss improved from 0.00070 to 0.00069, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 42/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 6.8042e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0769 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00042: loss improved from 0.00069 to 0.00068, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 43/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 6.7293e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0771 - val_categorical_accuracy: 0.9775\n",
            "\n",
            "Epoch 00043: loss improved from 0.00068 to 0.00067, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 44/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 6.6255e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0770 - val_categorical_accuracy: 0.9775\n",
            "\n",
            "Epoch 00044: loss improved from 0.00067 to 0.00066, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 45/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 6.5569e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0771 - val_categorical_accuracy: 0.9813\n",
            "\n",
            "Epoch 00045: loss improved from 0.00066 to 0.00066, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 46/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 6.4684e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0770 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00046: loss improved from 0.00066 to 0.00065, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 47/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 6.3990e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0785 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00047: loss improved from 0.00065 to 0.00064, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 48/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 6.3309e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0786 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00048: loss improved from 0.00064 to 0.00063, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 49/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 6.2584e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0780 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00049: loss improved from 0.00063 to 0.00063, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 50/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 6.1958e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0777 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00050: loss improved from 0.00063 to 0.00062, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 51/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 6.1320e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0786 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00051: loss improved from 0.00062 to 0.00061, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 52/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 6.0721e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0782 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00052: loss improved from 0.00061 to 0.00061, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 53/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 6.0130e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0789 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00053: loss improved from 0.00061 to 0.00060, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 54/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.9555e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0789 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00054: loss improved from 0.00060 to 0.00060, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 55/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.9024e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0792 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00055: loss improved from 0.00060 to 0.00059, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 56/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.8590e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0786 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00056: loss improved from 0.00059 to 0.00059, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 57/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.7997e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0789 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00057: loss improved from 0.00059 to 0.00058, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 58/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.7562e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0795 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00058: loss improved from 0.00058 to 0.00058, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 59/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.7053e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0792 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00059: loss improved from 0.00058 to 0.00057, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 60/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.6725e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0801 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00060: loss improved from 0.00057 to 0.00057, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 61/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.6204e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0801 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00061: loss improved from 0.00057 to 0.00056, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 62/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.5860e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0796 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00062: loss improved from 0.00056 to 0.00056, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 63/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.5370e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0803 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00063: loss improved from 0.00056 to 0.00055, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 64/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.5078e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0807 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00064: loss improved from 0.00055 to 0.00055, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 65/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.4621e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0795 - val_categorical_accuracy: 0.9813\n",
            "\n",
            "Epoch 00065: loss improved from 0.00055 to 0.00055, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 66/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.4266e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0807 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00066: loss improved from 0.00055 to 0.00054, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 67/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.3978e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0807 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00067: loss improved from 0.00054 to 0.00054, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 68/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.3515e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0803 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00068: loss improved from 0.00054 to 0.00054, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 69/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.3177e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0806 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00069: loss improved from 0.00054 to 0.00053, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 70/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.2824e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0811 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00070: loss improved from 0.00053 to 0.00053, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 71/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.2514e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0811 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00071: loss improved from 0.00053 to 0.00053, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 72/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.2207e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0814 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00072: loss improved from 0.00053 to 0.00052, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 73/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.1883e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0815 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00073: loss improved from 0.00052 to 0.00052, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 74/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.1637e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0808 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00074: loss improved from 0.00052 to 0.00052, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 75/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.1319e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0814 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00075: loss improved from 0.00052 to 0.00051, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 76/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.1016e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0814 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00076: loss improved from 0.00051 to 0.00051, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 77/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.0736e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0824 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00077: loss improved from 0.00051 to 0.00051, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 78/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.0496e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0824 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00078: loss improved from 0.00051 to 0.00050, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 79/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 5.0200e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0825 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00079: loss improved from 0.00050 to 0.00050, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 80/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.9960e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0825 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00080: loss improved from 0.00050 to 0.00050, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 81/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.9788e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0821 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00081: loss improved from 0.00050 to 0.00050, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 82/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.9470e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0818 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00082: loss improved from 0.00050 to 0.00049, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 83/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.9258e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0825 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00083: loss improved from 0.00049 to 0.00049, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 84/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.9028e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0825 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00084: loss improved from 0.00049 to 0.00049, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 85/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.8796e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0823 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00085: loss improved from 0.00049 to 0.00049, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 86/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.8564e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0825 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00086: loss improved from 0.00049 to 0.00049, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 87/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.8405e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0829 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00087: loss improved from 0.00049 to 0.00048, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 88/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.8164e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0832 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00088: loss improved from 0.00048 to 0.00048, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 89/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.7985e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0835 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00089: loss improved from 0.00048 to 0.00048, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 90/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.7758e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0829 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00090: loss improved from 0.00048 to 0.00048, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 91/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.7545e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0837 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00091: loss improved from 0.00048 to 0.00048, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 92/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.7384e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0835 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00092: loss improved from 0.00048 to 0.00047, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 93/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.7159e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0830 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00093: loss improved from 0.00047 to 0.00047, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 94/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.7016e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0836 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00094: loss improved from 0.00047 to 0.00047, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 95/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.6815e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0840 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00095: loss improved from 0.00047 to 0.00047, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 96/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.6629e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0839 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00096: loss improved from 0.00047 to 0.00047, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 97/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.6475e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0841 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00097: loss improved from 0.00047 to 0.00046, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 98/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.6298e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0840 - val_categorical_accuracy: 0.9800\n",
            "\n",
            "Epoch 00098: loss improved from 0.00046 to 0.00046, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 99/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.6139e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0844 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00099: loss improved from 0.00046 to 0.00046, saving model to vgg-Bilinearnet.h5\n",
            "Epoch 100/100\n",
            "3200/3200 [==============================] - 56s 17ms/step - loss: 4.5976e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0838 - val_categorical_accuracy: 0.9788\n",
            "\n",
            "Epoch 00100: loss improved from 0.00046 to 0.00046, saving model to vgg-Bilinearnet.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz8aQJktW7R6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "33881af2-a306-4f5b-c92d-fcd4241963c4"
      },
      "source": [
        "print(history.history.keys())\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['categorical_accuracy'])\n",
        "plt.plot(history.history['val_categorical_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_categorical_accuracy', 'loss', 'categorical_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWSksREQkK4WFiIhk9f8BMELHbz+SO9kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}